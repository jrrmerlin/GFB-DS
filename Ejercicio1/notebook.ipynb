{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1\n",
    "\n",
    "Construcción de un proceso ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la construcciòn del ETL se opto por el uso de AWS GLUE. \n",
    "Servicio tipo serverless que facilita la integración de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler\n",
    "El primer paso es crear un crawler. El cual es el enlace con nuestro DataStore. El crawler permite inferir el schema de la información persistida.\n",
    "\n",
    "1. En la consola de AWS buscamos AWS Glue.\n",
    "2. Damos clic en la parte de crawler.\n",
    "3. Clic en Add crawler.\n",
    "4. En crawler source type escogemos Data Stores y Crawl all folders.\n",
    "5. Como Data Store escogemos un bucket S3.\n",
    "6. Dejamos vacia la parte de connection.\n",
    "7. Seleccionamos Specified path in my account.\n",
    "8. En la parte path buscamos nuestro bucket-test-data y seleccionamos la carpeta raw.\n",
    "9. En Add Another Data Store seleccionamos No.\n",
    "10. Escogemos un IAM role adecuado. (AmazonS3FullAccess, AWSGlueServiceRole, AWSGlueConsoleFullAccess)\n",
    "11. En frequency por tema del ejercicio usaremos Run On Demand.\n",
    "12. El output del crawler será una base de datos en AWS GLUE. Seleccionamos Add Database.\n",
    "13. Le damos como nombre test-db.\n",
    "14. Damos Next.\n",
    "15. Damos Finish.\n",
    "16. Seleccionamos el crawler creado y la opción Run Crawler.\n",
    "17. Al terminar el status, navegamos a Databases/tables\n",
    "18. Veremos la tabla raw lo que indica que el crawler fue ejecutado con éxito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL\n",
    "El segundo paso es crear un ETL Job.\n",
    "1. Damos clic en ETL jobs.\n",
    "2. Clic en Add job.\n",
    "3. Damos un nombre al job, y escogemos el mismo rol usado previamente.\n",
    "4. Type Spark, Glue Version Spark 2.4, Python 3(Glue Version 2.0)\n",
    "5. En this job runs seleccionamos A proposed script generated by AWS Glue.\n",
    "6. Damos un nombre al script y una ruta S3 donde se almacenará. s3://anoc001-test-rodrigoruiz/AWS-Glue/glue-scripts\n",
    "7. Para el temporal s3://anoc001-test-rodrigoruiz/AWS-Glue/glue-tmp\n",
    "8. Escogemos un Data Source. En nuestro caso el de raw.\n",
    "9. Escogemos Change Schema.\n",
    "10. Escogemos la opción Create tables in your data target con los valores.\n",
    "    a. Data Store: S3\n",
    "    b. Format: Parquet\n",
    "    c. Target Path: s3://anoc001-test-data-rodrigoruiz/processed\n",
    "11. Se visualiza un mapeo de la información Source y Target. Damos en Save job and edit script.\n",
    "12. Se visualiza el script generado de forma automática el cual lee nuestra información en el bucket S3 y la transforma para persistirla en nuestro S3 destino.\n",
    "\n",
    "La sección anterior nos da un ETL listo para ejecutar. Sin embargo reemplazaremos el script por el proporcionado en el bucket en la misma sección. s3://anoc001-test-rodrigoruiz/AWS-Glue/glue-scripts/glue-test-job\n",
    "\n",
    "A continuación se explica el código agregado de forma manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al revisar la información y entender que solo se quiere los montos referentes a las ventas. Se hace un filtrado en los registros dada la condición status en paid o pending_payment. Entendiendo que los demás conceptos se requieren pero para un balance general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos los registros que son de tipo paid o pending_payment\n",
    "## @type: Filter\n",
    "## @args: [f = lambda x: x[\"status\"] in ['paid',\"pending_payment\"]]\n",
    "## @return: filtersales\n",
    "## @inputs: [frame = resolvechoice2]\n",
    "filtersales = Filter.apply(frame = resolvechoice2, f = lambda x: x[\"status\"] in ['paid',\"pending_payment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la limpieza de los datos se crearon 3 funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que completa el campo name\n",
    "def completeName(row):\n",
    "    if row[\"name\"] is None or row[\"name\"] == \"\":\n",
    "        if row[\"company_id\"] == \"cbf1c8b09cd5b549416d49d220a40cbd317f952e\":\n",
    "            row[\"name\"] = \"MiPasajefy\"\n",
    "        elif row[\"company_id\"] == \"8f642dc67fccf861548dfe1c761ce22f795e91f0\":\n",
    "            row[\"name\"] = \"Muebles chidos\"\n",
    "    return row\n",
    "\n",
    "# Funcion que completa el campo company_id            \n",
    "def completeCompanyId(row):\n",
    "    if row[\"company_id\"] is None or row[\"company_id\"] == \"\":\n",
    "        if row[\"name\"] == \"MiPasajefy\":\n",
    "            row[\"company_id\"] = \"cbf1c8b09cd5b549416d49d220a40cbd317f952e\"\n",
    "        elif row[\"name\"] == \"Muebles chidos\":\n",
    "            row[\"company_id\"] = \"8f642dc67fccf861548dfe1c761ce22f795e91f0\"\n",
    "    return row\n",
    "    \n",
    "# Funcion que limpia y estandariza el campo created_at\n",
    "def cleanCreatedAt(row):\n",
    "    if len(row[\"created_at\"])<9:\n",
    "        row[\"created_at\"]=row[\"created_at\"][6:]+\"/\"+row[\"created_at\"][4:6]+\"/\"+row[\"created_at\"][:4]\n",
    "    if \"-\" in row[\"created_at\"]:\n",
    "        row[\"created_at\"]=row[\"created_at\"][8:10]+\"/\"+row[\"created_at\"][5:7]+\"/\"+row[\"created_at\"][:4]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos las funciones a nuestro dynamicframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la función completeName    \n",
    "## @type: Map\n",
    "## @args: [f = completeName, transformation_ctx = \"completedName\"]\n",
    "## @return: completedName\n",
    "## @inputs: [frame = <frame>]\n",
    "completedName = Map.apply(frame = filtersales, f = completeName, transformation_ctx = \"completedName\")\n",
    "\n",
    "# Aplicamos la función completeCompanyId    \n",
    "## @type: Map\n",
    "## @args: [f = completeCompanyId, transformation_ctx = \"completeCompanyId\"]\n",
    "## @return: completedCompanyId\n",
    "## @inputs: [frame = completedName]\n",
    "completedCompanyId = Map.apply(frame = completedName, f = completeCompanyId, transformation_ctx = \"completeCompanyId\")\n",
    "\n",
    "# Aplicamos la función cleanCreatedAt    \n",
    "## @type: Map\n",
    "## @args: [f = cleanCreatedAt, transformation_ctx = \"cleanCreatedAt\"]\n",
    "## @return: cleanedCreatedAt\n",
    "## @inputs: [frame = completedCompanyId]\n",
    "cleanedCreatedAt = Map.apply(frame = completedCompanyId, f = cleanCreatedAt, transformation_ctx = \"cleanCreatedAt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por requisito se requiere persistir solo el sumarizado de ventas, agrupando por los campos name y created_at.\n",
    "Por lo que convertiremos nuestro dynamicframe en un dataframe para poderlo explotar de manera facil con SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un df para crear una vista temporal y agrupar via SQL las ventas por name y created_at\n",
    "df = cleanedCreatedAt.toDF()\n",
    "df.createOrReplaceTempView(\"salesTable\")\n",
    "result_sql_df=spark.sql(\"SELECT name, created_at, SUM(amount) FROM salesTable GROUP BY name, created_at\")\n",
    "# Lo pasamos a dynamicFrame para persistirlo\n",
    "result_dyf = DynamicFrame.fromDF(result_sql_df, glueContext, \"result_sql_df\")\n",
    "# Este coalesce es opcional y es para que quede un unico archivo en el procesamiento. Sin embargo puede afectar temas de performance.\n",
    "result_dyf_with_less_partitions=result_dyf.coalesce(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente Persistimos la información en nuestro S3. El cual será insumo de nuestro dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @type: DataSink\n",
    "## @args: [connection_type = \"s3\", connection_options = {\"path\": \"s3://anoc001-test-data-rodrigoruiz/processed\"}, format = \"parquet\", transformation_ctx = \"datasink4\"]\n",
    "## @return: datasink4\n",
    "## @inputs: [frame = result_dyf_with_less_partitions]\n",
    "datasink4 = glueContext.write_dynamic_frame.from_options(frame = result_dyf_with_less_partitions, connection_type = \"s3\", connection_options = {\"path\": \"s3://anoc001-test-data-rodrigoruiz/processed\"}, format = \"parquet\", transformation_ctx = \"datasink4\")\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Para ejecutar el job basta con Dar clic en Run job.\n",
    "Al terminar la ejecución podremos visualizar en el S3 el insumo requerido."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
